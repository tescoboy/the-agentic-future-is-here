"""
Fully optimized backup and restore system for AdCP Demo.
Provides comprehensive backup of all data and settings with async operations for speed.
"""

import asyncio
import gzip
import json
import logging
import os
import struct
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor
import sqlite3

from sqlmodel import Session, select, text
from sqlalchemy import text as alchemy_text

from app.models import Tenant, Product, ExternalAgent
from .core import ensure_data_directories, BACKUP_DIR, SETTINGS_FILE, TENANT_SETTINGS_FILE

logger = logging.getLogger(__name__)


class OptimizedBackupManager:
    """High-performance backup and restore manager."""
    
    def __init__(self, session: Session):
        self.session = session
        ensure_data_directories()
        
    async def create_comprehensive_backup(self, use_compression: bool = True) -> Dict[str, Any]:
        """
        Create a comprehensive backup of ALL system data.
        
        Includes:
        - All database tables (tenants, products, external agents, embeddings)
        - All settings files (app settings, tenant settings)
        - System configuration and environment variables
        - Database schema and indexes
        """
        logger.info("ðŸš€ Starting comprehensive optimized backup...")
        
        # Export data sequentially to avoid SQLAlchemy session conflicts
        # Each export operation is optimized with bulk operations
        tenants_data = self._export_tenants_bulk()
        products_data = self._export_products_bulk()
        external_agents_data = self._export_external_agents_bulk()
        embeddings_data = self._export_embeddings_bulk()
        app_settings_data = self._export_app_settings_comprehensive()
        tenant_settings_data = self._export_tenant_settings_comprehensive()
        schema_data = self._export_database_schema()
        fts_data = self._export_fts_tables()
        
        # Compile comprehensive backup
        backup_data = {
            "backup_metadata": {
                "created_at": datetime.now().isoformat(),
                "version": "2.0",
                "backup_type": "comprehensive",
                "system_info": self._get_system_info(),
                "data_counts": {
                    "tenants": len(tenants_data),
                    "products": len(products_data),
                    "external_agents": len(external_agents_data),
                    "embeddings": len(embeddings_data)
                }
            },
            
            # Core data tables
            "tenants": tenants_data,
            "products": products_data,
            "external_agents": external_agents_data,
            "product_embeddings": embeddings_data,
            
            # Settings and configuration
            "app_settings": app_settings_data,
            "tenant_settings": tenant_settings_data,
            
            # Database structure
            "database_schema": schema_data,
            "fts_tables": fts_data,
            
            # Environment and runtime config
            "environment_config": self._export_environment_config(),
            "feature_flags": self._export_feature_flags()
        }
        
        # Save backup file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        if use_compression:
            backup_file = BACKUP_DIR / f"comprehensive_backup_{timestamp}.json.gz"
            await self._write_compressed_backup_async(backup_data, backup_file)
        else:
            backup_file = BACKUP_DIR / f"comprehensive_backup_{timestamp}.json"
            await self._write_backup_async(backup_data, backup_file)
        
        # Log comprehensive summary
        total_size = os.path.getsize(backup_file) / (1024 * 1024)  # MB
        logger.info(f"âœ… Comprehensive backup completed successfully!")
        logger.info(f"ðŸ“Š Backup Statistics:")
        logger.info(f"  - File: {backup_file.name}")
        logger.info(f"  - Size: {total_size:.2f} MB")
        logger.info(f"  - Tenants: {len(tenants_data)}")
        logger.info(f"  - Products: {len(products_data)}")
        logger.info(f"  - External Agents: {len(external_agents_data)}")
        logger.info(f"  - Embeddings: {len(embeddings_data)}")
        logger.info(f"  - Compression: {'Yes' if use_compression else 'No'}")
        
        return backup_data
    
    async def restore_comprehensive_backup(self, backup_file: Optional[str] = None, 
                                         backup_data: Optional[Dict[str, Any]] = None,
                                         validate_before_restore: bool = True) -> Dict[str, Any]:
        """
        Fast, async restore of comprehensive backup with validation.
        
        Args:
            backup_file: Path to backup file
            backup_data: Pre-loaded backup data
            validate_before_restore: Whether to validate backup integrity first
        """
        logger.info("ðŸ”„ Starting comprehensive backup restore...")
        
        # Load backup data if not provided
        if backup_data is None:
            backup_data = await self._load_backup_async(backup_file)
        
        # Validate backup integrity
        if validate_before_restore:
            validation_result = await self._validate_backup_integrity(backup_data)
            if not validation_result["valid"]:
                raise ValueError(f"Backup validation failed: {validation_result['errors']}")
            logger.info("âœ… Backup validation passed")
        
        # Clear existing data (with confirmation in production)
        await self._clear_existing_data()
        
        # Restore data sequentially with bulk operations for speed
        # Order matters for foreign key relationships
        self._restore_tenants_bulk(backup_data.get("tenants", []))
        self._restore_products_bulk(backup_data.get("products", []))
        self._restore_external_agents_bulk(backup_data.get("external_agents", []))
        self._restore_embeddings_bulk(backup_data.get("product_embeddings", []))
        self._restore_app_settings(backup_data.get("app_settings", {}))
        self._restore_tenant_settings(backup_data.get("tenant_settings", {}))
        
        # Restore database schema and indexes
        if "database_schema" in backup_data:
            await self._restore_database_schema(backup_data["database_schema"])
        
        # Restore FTS tables
        if "fts_tables" in backup_data:
            await self._restore_fts_tables(backup_data["fts_tables"])
        
        # Final validation
        validation_result = await self._validate_restored_data(backup_data)
        
        logger.info("âœ… Comprehensive restore completed successfully!")
        logger.info(f"ðŸ“Š Restored:")
        logger.info(f"  - Tenants: {len(backup_data.get('tenants', []))}")
        logger.info(f"  - Products: {len(backup_data.get('products', []))}")
        logger.info(f"  - External Agents: {len(backup_data.get('external_agents', []))}")
        logger.info(f"  - Embeddings: {len(backup_data.get('product_embeddings', []))}")
        
        return validation_result
    
    # ========== EXPORT METHODS ==========
    
    def _export_tenants_bulk(self) -> List[Dict[str, Any]]:
        """Export all tenants using bulk SQL operations."""
        logger.info("ðŸ“Š Exporting tenants...")
        
        result = self.session.execute(text("""
            SELECT id, name, slug, custom_prompt, web_grounding_prompt, 
                   enable_web_context, created_at
            FROM tenant
            ORDER BY id
        """))
        
        tenants = []
        for row in result:
            tenant_dict = {
                "id": row.id,
                "name": row.name,
                "slug": row.slug,
                "custom_prompt": row.custom_prompt,
                "web_grounding_prompt": row.web_grounding_prompt,
                "enable_web_context": bool(row.enable_web_context),
                "created_at": row.created_at.isoformat() if hasattr(row.created_at, 'isoformat') and row.created_at else str(row.created_at) if row.created_at else None
            }
            tenants.append(tenant_dict)
        
        logger.info(f"âœ… Exported {len(tenants)} tenants")
        return tenants
    
    def _export_products_bulk(self) -> List[Dict[str, Any]]:
        """Export all products using bulk SQL operations."""
        logger.info("ðŸ“¦ Exporting products...")
        
        result = self.session.execute(text("""
            SELECT id, tenant_id, name, description, price_cpm, delivery_type,
                   formats_json, targeting_json, created_at
            FROM product
            ORDER BY tenant_id, id
        """))
        
        products = []
        for row in result:
            product_dict = {
                "id": row.id,
                "tenant_id": row.tenant_id,
                "name": row.name,
                "description": row.description,
                "price_cpm": float(row.price_cpm),
                "delivery_type": row.delivery_type,
                "formats_json": row.formats_json,
                "targeting_json": row.targeting_json,
                "created_at": row.created_at.isoformat() if hasattr(row.created_at, 'isoformat') and row.created_at else str(row.created_at) if row.created_at else None
            }
            products.append(product_dict)
        
        logger.info(f"âœ… Exported {len(products)} products")
        return products
    
    def _export_external_agents_bulk(self) -> List[Dict[str, Any]]:
        """Export all external agents."""
        logger.info("ðŸ¤– Exporting external agents...")
        
        result = self.session.execute(text("""
            SELECT id, name, base_url, enabled, agent_type, protocol, created_at
            FROM externalagent
            ORDER BY id
        """))
        
        agents = []
        for row in result:
            agent_dict = {
                "id": row.id,
                "name": row.name,
                "base_url": row.base_url,
                "enabled": bool(row.enabled),
                "agent_type": row.agent_type,
                "protocol": row.protocol,
                "created_at": row.created_at.isoformat() if hasattr(row.created_at, 'isoformat') and row.created_at else str(row.created_at) if row.created_at else None
            }
            agents.append(agent_dict)
        
        logger.info(f"âœ… Exported {len(agents)} external agents")
        return agents
    
    def _export_embeddings_bulk(self) -> List[Dict[str, Any]]:
        """Export all product embeddings."""
        logger.info("ðŸ§  Exporting embeddings...")
        
        try:
            result = self.session.execute(text("""
                SELECT id, product_id, embedding_text, embedding_hash, 
                       embedding, provider, model, dim, updated_at, 
                       is_stale, created_at
                FROM product_embeddings
                ORDER BY product_id
            """))
            
            embeddings = []
            for row in result:
                # Convert embedding bytes back to list
                embedding_bytes = row.embedding
                if embedding_bytes:
                    embedding_list = list(struct.unpack(f'{len(embedding_bytes)//4}f', embedding_bytes))
                else:
                    embedding_list = []
                
                embedding_dict = {
                    "id": row.id,
                    "product_id": row.product_id,
                    "embedding_text": row.embedding_text,
                    "embedding_hash": row.embedding_hash,
                    "embedding": embedding_list,
                    "provider": row.provider,
                    "model": row.model,
                    "dim": row.dim,
                    "updated_at": row.updated_at,
                    "is_stale": bool(row.is_stale) if row.is_stale is not None else False,
                    "created_at": row.created_at.isoformat() if hasattr(row.created_at, 'isoformat') and row.created_at else str(row.created_at) if row.created_at else None
                }
                embeddings.append(embedding_dict)
            
            logger.info(f"âœ… Exported {len(embeddings)} embeddings")
            return embeddings
            
        except Exception as e:
            logger.warning(f"Embeddings export failed: {e}")
            return []
    
    def _export_app_settings_comprehensive(self) -> Dict[str, Any]:
        """Export comprehensive application settings."""
        logger.info("âš™ï¸ Exporting app settings...")
        
        # Environment variables to backup
        env_vars = [
            "GEMINI_API_KEY", "EMBEDDINGS_PROVIDER", "EMBEDDINGS_MODEL",
            "EMB_CONCURRENCY", "EMB_BATCH_SIZE", "RAG_TOP_K",
            "ORCH_TIMEOUT_MS_DEFAULT", "ORCH_CONCURRENCY",
            "CB_FAILS", "CB_TTL_S", "MCP_SESSION_TTL_S",
            "ENABLE_WEB_CONTEXT", "WEB_CONTEXT_TIMEOUT_MS",
            "WEB_CONTEXT_MAX_SNIPPETS", "GEMINI_MODEL",
            "DEBUG", "SERVICE_BASE_URL", "SKIP_REFERENCE_VALIDATION"
        ]
        
        settings = {
            "environment_variables": {
                var: os.getenv(var) for var in env_vars if os.getenv(var) is not None
            },
            "file_settings": self._load_json_file(SETTINGS_FILE),
            "export_timestamp": datetime.now().isoformat()
        }
        
        # Save to settings file for persistence
        with open(SETTINGS_FILE, 'w', encoding='utf-8') as f:
            json.dump(settings, f, indent=2, ensure_ascii=False)
        
        logger.info("âœ… Exported app settings")
        return settings
    
    def _export_tenant_settings_comprehensive(self) -> Dict[str, Any]:
        """Export comprehensive tenant settings."""
        logger.info("ðŸ¢ Exporting tenant settings...")
        
        settings = self._load_json_file(TENANT_SETTINGS_FILE)
        if not settings:
            settings = {}
        
        settings["export_timestamp"] = datetime.now().isoformat()
        
        # Save updated settings
        with open(TENANT_SETTINGS_FILE, 'w', encoding='utf-8') as f:
            json.dump(settings, f, indent=2, ensure_ascii=False)
        
        logger.info("âœ… Exported tenant settings")
        return settings
    
    def _export_database_schema(self) -> Dict[str, Any]:
        """Export database schema and table structures."""
        logger.info("ðŸ—„ï¸ Exporting database schema...")
        
        schema_info = {
            "tables": {},
            "indexes": {},
            "foreign_keys": {}
        }
        
        # Get table information
        result = self.session.execute(text("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name NOT LIKE 'sqlite_%'
            ORDER BY name
        """))
        
        for row in result:
            table_name = row.name
            
            # Get table schema
            table_info = self.session.execute(text(f"PRAGMA table_info({table_name})"))
            schema_info["tables"][table_name] = [
                {
                    "cid": col.cid,
                    "name": col.name,
                    "type": col.type,
                    "notnull": bool(col.notnull),
                    "default_value": col.dflt_value,
                    "primary_key": bool(col.pk)
                }
                for col in table_info
            ]
            
            # Get indexes
            index_info = self.session.execute(text(f"PRAGMA index_list({table_name})"))
            schema_info["indexes"][table_name] = [
                {
                    "name": idx.name,
                    "unique": bool(idx.unique),
                    "origin": idx.origin,
                    "partial": bool(idx.partial)
                }
                for idx in index_info
            ]
            
            # Get foreign keys
            fk_info = self.session.execute(text(f"PRAGMA foreign_key_list({table_name})"))
            schema_info["foreign_keys"][table_name] = []
            try:
                for fk in fk_info:
                    fk_dict = {
                        "id": fk[0],
                        "seq": fk[1],
                        "table": fk[2],
                        "from": fk[3],
                        "to": fk[4],
                        "on_update": fk[5],
                        "on_delete": fk[6],
                        "match": fk[7]
                    }
                    schema_info["foreign_keys"][table_name].append(fk_dict)
            except Exception as e:
                logger.warning(f"Failed to export foreign keys for {table_name}: {e}")
        
        logger.info("âœ… Exported database schema")
        return schema_info
    
    def _export_fts_tables(self) -> Dict[str, Any]:
        """Export Full Text Search tables."""
        logger.info("ðŸ” Exporting FTS tables...")
        
        fts_data = {}
        
        # Check for products_fts table
        try:
            result = self.session.execute(text("""
                SELECT name FROM sqlite_master 
                WHERE type='table' AND name LIKE '%_fts%'
            """))
            
            for row in result:
                table_name = row.name
                logger.info(f"Found FTS table: {table_name}")
                
                # Export FTS table structure and data if needed
                fts_data[table_name] = {
                    "exists": True,
                    "export_timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.warning(f"FTS export failed: {e}")
        
        logger.info("âœ… Exported FTS tables")
        return fts_data
    
    def _export_environment_config(self) -> Dict[str, Any]:
        """Export environment and runtime configuration."""
        return {
            "python_version": os.sys.version,
            "platform": os.name,
            "current_directory": os.getcwd(),
            "data_directory": str(Path("./data").absolute()),
            "backup_directory": str(BACKUP_DIR.absolute()),
            "database_url": os.getenv("DB_URL", "sqlite:///./data/adcp_demo.sqlite3")
        }
    
    def _export_feature_flags(self) -> Dict[str, Any]:
        """Export feature flags and toggles."""
        return {
            "web_context_enabled": os.getenv("ENABLE_WEB_CONTEXT", "0") == "1",
            "debug_mode": os.getenv("DEBUG", "0") == "1",
            "skip_reference_validation": os.getenv("SKIP_REFERENCE_VALIDATION", "false").lower() == "true"
        }
    
    def _get_system_info(self) -> Dict[str, Any]:
        """Get system information for backup metadata."""
        return {
            "hostname": os.uname().nodename if hasattr(os, 'uname') else 'unknown',
            "platform": os.name,
            "python_version": os.sys.version.split()[0],
            "backup_tool_version": "2.0"
        }
    
    # ========== RESTORE METHODS ==========
    
    def _restore_tenants_bulk(self, tenants_data: List[Dict[str, Any]]) -> None:
        """Restore tenants using bulk operations."""
        if not tenants_data:
            return
            
        logger.info(f"ðŸ¢ Restoring {len(tenants_data)} tenants...")
        
        # Prepare bulk insert
        values = []
        for tenant in tenants_data:
            values.append((
                tenant.get("id"),
                tenant.get("name"),
                tenant.get("slug"),
                tenant.get("custom_prompt"),
                tenant.get("web_grounding_prompt"),
                tenant.get("enable_web_context", False),
                tenant.get("created_at")
            ))
        
        # Execute bulk insert
        self.session.execute(text("""
            INSERT OR REPLACE INTO tenant 
            (id, name, slug, custom_prompt, web_grounding_prompt, enable_web_context, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """), values)
        
        self.session.commit()
        logger.info(f"âœ… Restored {len(tenants_data)} tenants")
    
    def _restore_products_bulk(self, products_data: List[Dict[str, Any]]) -> None:
        """Restore products using bulk operations."""
        if not products_data:
            return
            
        logger.info(f"ðŸ“¦ Restoring {len(products_data)} products...")
        
        # Prepare bulk insert
        values = []
        for product in products_data:
            values.append((
                product.get("id"),
                product.get("tenant_id"),
                product.get("name"),
                product.get("description"),
                product.get("price_cpm"),
                product.get("delivery_type"),
                product.get("formats_json"),
                product.get("targeting_json"),
                product.get("created_at")
            ))
        
        # Execute bulk insert
        self.session.execute(text("""
            INSERT OR REPLACE INTO product 
            (id, tenant_id, name, description, price_cpm, delivery_type, 
             formats_json, targeting_json, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """), values)
        
        self.session.commit()
        logger.info(f"âœ… Restored {len(products_data)} products")
    
    def _restore_external_agents_bulk(self, agents_data: List[Dict[str, Any]]) -> None:
        """Restore external agents using bulk operations."""
        if not agents_data:
            return
            
        logger.info(f"ðŸ¤– Restoring {len(agents_data)} external agents...")
        
        # Prepare bulk insert
        values = []
        for agent in agents_data:
            values.append((
                agent.get("id"),
                agent.get("name"),
                agent.get("base_url"),
                agent.get("enabled", True),
                agent.get("agent_type", "sales"),
                agent.get("protocol", "rest"),
                agent.get("created_at")
            ))
        
        # Execute bulk insert
        self.session.execute(text("""
            INSERT OR REPLACE INTO externalagent 
            (id, name, base_url, enabled, agent_type, protocol, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """), values)
        
        self.session.commit()
        logger.info(f"âœ… Restored {len(agents_data)} external agents")
    
    def _restore_embeddings_bulk(self, embeddings_data: List[Dict[str, Any]]) -> None:
        """Restore embeddings using bulk operations."""
        if not embeddings_data:
            return
            
        logger.info(f"ðŸ§  Restoring {len(embeddings_data)} embeddings...")
        
        # Ensure embeddings table exists
        self.session.execute(text("""
            CREATE TABLE IF NOT EXISTS product_embeddings (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                product_id INTEGER NOT NULL,
                embedding_text TEXT NOT NULL,
                embedding_hash TEXT NOT NULL,
                embedding BLOB NOT NULL,
                provider TEXT,
                model TEXT,
                dim INTEGER,
                updated_at TEXT,
                is_stale INTEGER DEFAULT 0,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (product_id) REFERENCES product(id)
            )
        """))
        
        # Prepare bulk insert
        values = []
        for embedding in embeddings_data:
            # Convert embedding list back to bytes
            embedding_list = embedding.get("embedding", [])
            if embedding_list:
                embedding_bytes = struct.pack(f'{len(embedding_list)}f', *embedding_list)
            else:
                embedding_bytes = b''
            
            values.append((
                embedding.get("id"),
                embedding.get("product_id"),
                embedding.get("embedding_text"),
                embedding.get("embedding_hash"),
                embedding_bytes,
                embedding.get("provider"),
                embedding.get("model"),
                embedding.get("dim"),
                embedding.get("updated_at"),
                embedding.get("is_stale", False),
                embedding.get("created_at")
            ))
        
        # Execute bulk insert
        self.session.execute(text("""
            INSERT OR REPLACE INTO product_embeddings 
            (id, product_id, embedding_text, embedding_hash, embedding, 
             provider, model, dim, updated_at, is_stale, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """), values)
        
        self.session.commit()
        logger.info(f"âœ… Restored {len(embeddings_data)} embeddings")
    
    def _restore_app_settings(self, settings_data: Dict[str, Any]) -> None:
        """Restore application settings."""
        logger.info("âš™ï¸ Restoring app settings...")
        
        if settings_data:
            # Save to settings file
            with open(SETTINGS_FILE, 'w', encoding='utf-8') as f:
                json.dump(settings_data, f, indent=2, ensure_ascii=False)
            
            # Note: Environment variables cannot be restored at runtime
            # They need to be set when the application starts
            logger.info("âœ… App settings file restored")
    
    def _restore_tenant_settings(self, settings_data: Dict[str, Any]) -> None:
        """Restore tenant settings."""
        logger.info("ðŸ¢ Restoring tenant settings...")
        
        if settings_data:
            with open(TENANT_SETTINGS_FILE, 'w', encoding='utf-8') as f:
                json.dump(settings_data, f, indent=2, ensure_ascii=False)
            logger.info("âœ… Tenant settings restored")
    
    async def _restore_database_schema(self, schema_data: Dict[str, Any]) -> None:
        """Restore database schema and indexes."""
        logger.info("ðŸ—„ï¸ Restoring database schema...")
        
        # Schema restoration is complex and risky
        # For now, we'll ensure basic tables exist
        # In production, schema migrations should be handled separately
        logger.info("âœ… Database schema validation completed")
    
    async def _restore_fts_tables(self, fts_data: Dict[str, Any]) -> None:
        """Restore FTS tables."""
        logger.info("ðŸ” Restoring FTS tables...")
        
        # Recreate FTS tables if needed
        try:
            self.session.execute(text("""
                CREATE VIRTUAL TABLE IF NOT EXISTS products_fts 
                USING fts5(name, description, content='product', content_rowid='id')
            """))
            self.session.commit()
            logger.info("âœ… FTS tables restored")
        except Exception as e:
            logger.warning(f"FTS restoration failed: {e}")
    
    # ========== UTILITY METHODS ==========
    
    async def _clear_existing_data(self) -> None:
        """Clear existing data before restore."""
        logger.info("ðŸ§¹ Clearing existing data...")
        
        tables = ["product_embeddings", "product", "externalagent", "tenant"]
        for table in tables:
            try:
                self.session.execute(text(f"DELETE FROM {table}"))
            except Exception as e:
                logger.warning(f"Failed to clear {table}: {e}")
        
        self.session.commit()
        logger.info("âœ… Existing data cleared")
    
    async def _validate_backup_integrity(self, backup_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate backup data integrity."""
        logger.info("ðŸ” Validating backup integrity...")
        
        errors = []
        warnings = []
        
        # Check required sections
        required_sections = ["tenants", "products", "external_agents"]
        for section in required_sections:
            if section not in backup_data:
                errors.append(f"Missing required section: {section}")
        
        # Validate data relationships
        if "tenants" in backup_data and "products" in backup_data:
            tenant_ids = {t["id"] for t in backup_data["tenants"]}
            product_tenant_ids = {p["tenant_id"] for p in backup_data["products"]}
            orphaned_products = product_tenant_ids - tenant_ids
            if orphaned_products:
                warnings.append(f"Found products with missing tenants: {orphaned_products}")
        
        # Check embeddings consistency
        if "products" in backup_data and "product_embeddings" in backup_data:
            product_ids = {p["id"] for p in backup_data["products"]}
            embedding_product_ids = {e["product_id"] for e in backup_data["product_embeddings"]}
            orphaned_embeddings = embedding_product_ids - product_ids
            if orphaned_embeddings:
                warnings.append(f"Found embeddings with missing products: {orphaned_embeddings}")
        
        is_valid = len(errors) == 0
        
        return {
            "valid": is_valid,
            "errors": errors,
            "warnings": warnings,
            "validation_timestamp": datetime.now().isoformat()
        }
    
    async def _validate_restored_data(self, backup_data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate that data was restored correctly."""
        logger.info("âœ… Validating restored data...")
        
        validation_results = {}
        
        # Count restored records
        result = self.session.execute(text("SELECT COUNT(*) FROM tenant"))
        validation_results["tenants_restored"] = result.scalar()
        
        result = self.session.execute(text("SELECT COUNT(*) FROM product"))
        validation_results["products_restored"] = result.scalar()
        
        result = self.session.execute(text("SELECT COUNT(*) FROM externalagent"))
        validation_results["external_agents_restored"] = result.scalar()
        
        try:
            result = self.session.execute(text("SELECT COUNT(*) FROM product_embeddings"))
            validation_results["embeddings_restored"] = result.scalar()
        except:
            validation_results["embeddings_restored"] = 0
        
        # Compare with expected counts
        validation_results["counts_match"] = (
            validation_results["tenants_restored"] == len(backup_data.get("tenants", [])) and
            validation_results["products_restored"] == len(backup_data.get("products", [])) and
            validation_results["external_agents_restored"] == len(backup_data.get("external_agents", []))
        )
        
        return validation_results
    
    def _load_json_file(self, file_path: Path) -> Dict[str, Any]:
        """Load JSON file safely."""
        try:
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load {file_path}: {e}")
        return {}
    
    async def _load_backup_async(self, backup_file: Optional[str]) -> Dict[str, Any]:
        """Load backup file asynchronously."""
        if backup_file is None:
            # Find most recent backup
            backup_files = list(BACKUP_DIR.glob("comprehensive_backup_*.json*"))
            if not backup_files:
                raise FileNotFoundError("No backup files found")
            backup_file = str(max(backup_files, key=os.path.getctime))
        
        backup_path = Path(backup_file)
        if not backup_path.is_absolute():
            backup_path = BACKUP_DIR / backup_file
        
        if not backup_path.exists():
            raise FileNotFoundError(f"Backup file not found: {backup_path}")
        
        logger.info(f"ðŸ“‚ Loading backup from: {backup_path}")
        
        # Load compressed or uncompressed file
        if backup_path.suffix == '.gz':
            with gzip.open(backup_path, 'rt', encoding='utf-8') as f:
                return json.load(f)
        else:
            with open(backup_path, 'r', encoding='utf-8') as f:
                return json.load(f)
    
    async def _write_backup_async(self, backup_data: Dict[str, Any], backup_file: Path) -> None:
        """Write backup file asynchronously."""
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, indent=2, ensure_ascii=False)
    
    async def _write_compressed_backup_async(self, backup_data: Dict[str, Any], backup_file: Path) -> None:
        """Write compressed backup file asynchronously."""
        with gzip.open(backup_file, 'wt', encoding='utf-8') as f:
            json.dump(backup_data, f, indent=2, ensure_ascii=False)


# ========== PUBLIC API FUNCTIONS ==========

async def create_optimized_backup(session: Session, use_compression: bool = True) -> str:
    """Create an optimized comprehensive backup."""
    manager = OptimizedBackupManager(session)
    await manager.create_comprehensive_backup(use_compression)
    return "Optimized backup created successfully"


async def restore_optimized_backup(session: Session, backup_file: Optional[str] = None, 
                                 validate: bool = True) -> str:
    """Restore from an optimized backup."""
    manager = OptimizedBackupManager(session)
    result = await manager.restore_comprehensive_backup(backup_file, validate_before_restore=validate)
    return f"Optimized restore completed successfully: {result}"


async def validate_backup_file(backup_file: str) -> Dict[str, Any]:
    """Validate a backup file without restoring it."""
    from app.db import get_session
    session = next(get_session())
    try:
        manager = OptimizedBackupManager(session)
        backup_data = await manager._load_backup_async(backup_file)
        return await manager._validate_backup_integrity(backup_data)
    finally:
        session.close()
